{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae8ecef5-4cf3-4ec5-9a1f-7eaa6558e488",
      "metadata": {
        "id": "ae8ecef5-4cf3-4ec5-9a1f-7eaa6558e488"
      },
      "source": [
        "# AI4ER Induction Week - Applied ML session 3\n",
        "### Designing a machine learning algorithm for predicting extreme weather events in the [ClimateNet](https://gmd.copernicus.org/articles/14/107/2021/) dataset\n",
        "\n",
        "##### The goal of this session is to experiment with using a deep neural network with the aim of trying and predict whether a given geographical location is experiencing an extreme weather event in the form of an atmospheric river or a tropical cyclone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3b980e",
      "metadata": {},
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a28fd9a-0a12-47a1-a6fa-8620e0231481",
      "metadata": {
        "id": "4a28fd9a-0a12-47a1-a6fa-8620e0231481"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import xarray as xr\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71cc179",
      "metadata": {},
      "source": [
        "The first thing to do is to create a training and testing dataset.\n",
        "\n",
        " - If using Colab, you must have first added a shortcut to your GDrive - see [here](https://drive.google.com/drive/folders/1vrLE8nbpMHdTeWCpI2rrS1scQmQW_ujE?usp=share_link): \n",
        " - If using your own machine, you must have downloaded the ClimateNet dataset from the above link and have it in the same directory as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4536d511-4b95-49ec-99f9-bb7fabb53b35",
      "metadata": {
        "id": "4536d511-4b95-49ec-99f9-bb7fabb53b35"
      },
      "outputs": [],
      "source": [
        "ds = xr.open_dataset('/content/drive/MyDrive/climate_net/climatenet_data.nc')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044e6d56",
      "metadata": {},
      "source": [
        "View the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5a95cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd59710c-537c-4c13-b51e-50cf3a7f1013",
      "metadata": {
        "id": "dd59710c-537c-4c13-b51e-50cf3a7f1013"
      },
      "source": [
        "Let us start by choosing some training inputs. You might want to do some data exploration to see what these they look like, and an idea of how they are related to each other. We also define our labels, which our network is trying to predict - 0 is no extreme event, 1 is an tropical cyclone and 2 is a atmospheric river."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44b4a86-ebb4-4520-81c3-25dd9fc7f57f",
      "metadata": {
        "id": "d44b4a86-ebb4-4520-81c3-25dd9fc7f57f"
      },
      "outputs": [],
      "source": [
        "input_1 = ds.U850.data\n",
        "input_2 = ds.V850.data\n",
        "input_3 = ds.TMQ.data\n",
        "input_4 = ds.PSL.data\n",
        "labels = ds.LABELS.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae91911-cbb5-4a8d-96f3-a11dabf7291e",
      "metadata": {
        "id": "7ae91911-cbb5-4a8d-96f3-a11dabf7291e"
      },
      "source": [
        "To make the problem more tractable, let's sparse down the data a little by filtering it.\n",
        "\n",
        "*Note: is this the best way to filter the data? What happens to the performance of the model if we just sparse regularly, for example? These are questions you might come back to later. Convolutional networks are a special class of neural network that essentially optimise a filtering process to sequentially reduce dimensionaility whilst extracting relevant spatial information from images.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33ec4ae-7d10-4b36-ba96-15fea4a18c30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33ec4ae-7d10-4b36-ba96-15fea4a18c30",
        "outputId": "48dac9e4-c807-4a5b-bf3f-dba6b69330c7"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage.filters import gaussian_filter\n",
        "filter_input_1 = gaussian_filter(input_1,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_2 = gaussian_filter(input_2,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_3 = gaussian_filter(input_3,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_4 = gaussian_filter(input_4,sigma = [0,10,10])[:,::16,::16]\n",
        "\n",
        "print(filter_input_1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6bc0fe-375e-42f6-a98c-e668ad5daeb9",
      "metadata": {
        "id": "6b6bc0fe-375e-42f6-a98c-e668ad5daeb9"
      },
      "source": [
        "Now we have 48 x 72 = 3456 inputs per time step. We filter the labels slightly differently, i.e. a filtered cell is labelled as an extreme event according to its Gaussian centre cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd3c602-3c06-4213-9c0a-ccf2091bbc7e",
      "metadata": {
        "id": "8cd3c602-3c06-4213-9c0a-ccf2091bbc7e"
      },
      "outputs": [],
      "source": [
        "filter_labels = labels[:,::16,::16]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cfac8b-fb09-4668-8dc8-1ca80ae38448",
      "metadata": {
        "id": "46cfac8b-fb09-4668-8dc8-1ca80ae38448"
      },
      "source": [
        "At this point the data is still in a nice form (i.e. we could do a contour plot for each timestep). Let us choose the first 67 time steps for training, and the remaining 16 for testing. We then need to convert it into the shape (training samples, number of inputs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6665635-28f6-4204-8756-71e6012d9b9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6665635-28f6-4204-8756-71e6012d9b9b",
        "outputId": "9ebc1848-34f8-4363-8f8a-c780ed10ed92"
      },
      "outputs": [],
      "source": [
        "X_train = np.stack((\n",
        "        filter_input_1[:67].flatten(),\n",
        "        filter_input_2[:67].flatten(),\n",
        "        filter_input_3[:67].flatten(),\n",
        "        filter_input_4[:67].flatten()\n",
        "        ),\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "X_test = np.stack((\n",
        "        filter_input_1[67:].flatten(),\n",
        "        filter_input_2[67:].flatten(),\n",
        "        filter_input_3[67:].flatten(),\n",
        "        filter_input_4[67:].flatten()\n",
        "        ),\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2811a4e6",
      "metadata": {},
      "source": [
        "We shall use one-hot encoded vectors for our labels, as to not imply any ordering in the data. This can be done using the identity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yESedjhzQdfj",
      "metadata": {
        "id": "yESedjhzQdfj"
      },
      "outputs": [],
      "source": [
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes)[y]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91838fb5-203b-4a77-b448-b1b7db013646",
      "metadata": {
        "id": "91838fb5-203b-4a77-b448-b1b7db013646"
      },
      "source": [
        "We need to similarly flatten the labels into the shape (training samples, number of outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e690f12e-5ca3-4f9f-bdd3-b5e3340541b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e690f12e-5ca3-4f9f-bdd3-b5e3340541b1",
        "outputId": "832ce5fe-b799-4969-efda-a5856e020db3"
      },
      "outputs": [],
      "source": [
        "train_labels = filter_labels[:67].flatten()\n",
        "test_labels  = filter_labels[67:].flatten()\n",
        "\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "nb_classes= 3\n",
        "Y_train = to_categorical(train_labels, num_classes=nb_classes)\n",
        "Y_test = to_categorical(test_labels  , num_classes=nb_classes)\n",
        "\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b682c0cd-6856-4c30-b289-b7804fc19787",
      "metadata": {
        "id": "b682c0cd-6856-4c30-b289-b7804fc19787"
      },
      "source": [
        "The final step for preparing the training data is to normalise it. This is particularly important for our dataset since the range of values varies greatly for the different inputs (for example, PSL compared to TMQ). Here, we normalise by subtracting the mean and dividing by the standard deviation. Remember to normalize the test data by the same mean and standard deviation as the training data.\n",
        "\n",
        "*You might want to experiment with different ways of normalizing the data, or look at what happens when there is no normalization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923c525a-30b1-461f-9d2d-5d65e8007dc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923c525a-30b1-461f-9d2d-5d65e8007dc8",
        "outputId": "09cff4ac-c4be-486c-8266-974b00028472"
      },
      "outputs": [],
      "source": [
        "mu    = np.mean(X_train, axis=0)\n",
        "sigma = np.std( X_train, axis=0)\n",
        "\n",
        "print(mu, sigma)\n",
        "\n",
        "X_train = (X_train-mu)/sigma #Careful not to run this cell twice\n",
        "X_test  = (X_test -mu)/sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ba6ab8",
      "metadata": {},
      "source": [
        "To input the data to the network in batches we define a data generator. This is a function that returns a batch of data each time it is called. This is useful for large datasets that cannot fit into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHAn06zhWJII",
      "metadata": {
        "id": "HHAn06zhWJII"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data   = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data   = self.data[idx]\n",
        "        labels = self.labels[idx]\n",
        "        return data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9925cd",
      "metadata": {},
      "source": [
        "Convert our data to tensors and create a data generator for the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26I7GBZLQ1Tv",
      "metadata": {
        "id": "26I7GBZLQ1Tv"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "\n",
        "val_split_percent = 5\n",
        "val_split         = int(X_train.size(0)*(1-val_split_percent/100))\n",
        "\n",
        "X_train, X_val = X_train[:val_split], X_train[val_split:]\n",
        "Y_train, Y_val = Y_train[:val_split], Y_train[val_split:]\n",
        "\n",
        "batch_size  = 128\n",
        "train_steps = int(X_train.size(0)/batch_size)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset(X_train, Y_train),\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = True\n",
        "    )\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset(X_val , Y_val),\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "797a370f-4e31-4a83-aee3-d23d1dc12a6e",
      "metadata": {
        "id": "797a370f-4e31-4a83-aee3-d23d1dc12a6e"
      },
      "source": [
        "Now that the data is prepared, let's build our first model. We will use a simple fully connected neural network with 2 hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf37d99-1569-4698-8765-046bda136dc0",
      "metadata": {
        "id": "5bf37d99-1569-4698-8765-046bda136dc0"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_size, nb_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 200),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(200, 60),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(60, nb_classes), # classifying into 3 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# View model architecture and parameters\n",
        "print(summary(Model(4, 3), (1, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5bb5bd7-f875-4237-beb2-0d7eef1b6325",
      "metadata": {
        "id": "f5bb5bd7-f875-4237-beb2-0d7eef1b6325"
      },
      "source": [
        "We define a training loop, that trains the model for a given number of epochs. We also print the loss history at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa2551d-cf7e-4f4e-8155-b8ef624e633a",
      "metadata": {
        "id": "7aa2551d-cf7e-4f4e-8155-b8ef624e633a"
      },
      "outputs": [],
      "source": [
        "def fit(network,\n",
        "        dataloader,\n",
        "        val_dataloader,\n",
        "        criterion,\n",
        "        optimiser,\n",
        "        epochs,\n",
        "        train_steps):\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # progress bar for loss tracking\n",
        "        with tqdm.trange(train_steps, ncols=100) as pbar:\n",
        "\n",
        "            ### TRAINING LOOP ###\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for steps, data in zip(pbar, dataloader):\n",
        "\n",
        "                inputs, labels = data              # data is a list of (inputs, labels)\n",
        "\n",
        "                outputs = network(inputs)          # forward pass\n",
        "\n",
        "                loss = criterion(outputs, labels)  # compute loss\n",
        "\n",
        "                loss.backward()                    # compute backward gradients\n",
        "\n",
        "                optimiser.step()                   # update the parameters\n",
        "\n",
        "                optimiser.zero_grad()              # zero the parameter gradients\n",
        "\n",
        "                running_loss += loss.item()        # track loss\n",
        "\n",
        "                if (steps+1)!=train_steps:         # if in training loop\n",
        "                    # print loss tracker\n",
        "                    pbar.set_postfix({\n",
        "                        'epoch'      : f\"{epoch+1}/{epochs}\",\n",
        "                        'loss'       : f\"{running_loss/(steps+1):.3f}\",\n",
        "                        'val loss'   : f\"-----\",\n",
        "                        })\n",
        "                \n",
        "                ### VALIDATION LOOP ###\n",
        "\n",
        "                else:\n",
        "\n",
        "                    val_steps        = 0\n",
        "                    running_val_loss = 0.0\n",
        "                    \n",
        "                    for inputs, labels in val_dataloader:\n",
        "\n",
        "                        # compute validation loss (do not track gradients)\n",
        "                        with torch.no_grad(): loss = criterion(network(inputs), labels)\n",
        "\n",
        "                        # track validation loss\n",
        "                        running_val_loss += loss.item()\n",
        "                        val_steps        += 1\n",
        "\n",
        "                    # print loss tracker for validation set\n",
        "                    pbar.set_postfix({\n",
        "                        'epoch'      : f\"{epoch+1}/{epochs}\",\n",
        "                        'loss'       : f\"{running_loss    /(steps+1)    :.3f}\",\n",
        "                        'val loss'   : f\"{running_val_loss/(val_steps+1):.3f}\",\n",
        "                        })\n",
        "                    pbar.update()\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e105970",
      "metadata": {},
      "source": [
        "We instantiate the model and define our loss function and optimiser. As well as the number of epochs to train for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034226aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "net       = Model(4, 3)\n",
        "epochs    = 10\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(net.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8cebeb",
      "metadata": {},
      "source": [
        "And train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5N4wquSGjrdv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N4wquSGjrdv",
        "outputId": "49282310-b6a4-43c8-e143-052c3c3c838b"
      },
      "outputs": [],
      "source": [
        "net = fit(\n",
        "    network        = net,\n",
        "    dataloader     = train_dataloader,\n",
        "    val_dataloader = val_dataloader,\n",
        "    criterion      = criterion,\n",
        "    optimiser      = optimiser,\n",
        "    epochs         = epochs,\n",
        "    train_steps    = train_steps\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1726aad3-d720-4018-9b97-a4e51d7f6ca1",
      "metadata": {
        "id": "1726aad3-d720-4018-9b97-a4e51d7f6ca1"
      },
      "source": [
        "We've trained our first model for making predictions on the data. Now let's have a look at how the model performs on the test set. We'll start by doing things qualitatively: reshaping the predicted classes back into their original spatio-temporal form and then plotting some time slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff12b66d-e53e-4075-89f3-c69e2d41aa6e",
      "metadata": {
        "id": "ff12b66d-e53e-4075-89f3-c69e2d41aa6e"
      },
      "outputs": [],
      "source": [
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predicted_classes = net(X_test).numpy()\n",
        "    predicted_labels = np.argmax(predicted_classes, axis=1).reshape((16, 48, 72))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7605fe-1834-41b0-9037-73ffc6828db9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "df7605fe-1834-41b0-9037-73ffc6828db9",
        "outputId": "f478492e-3fa2-494a-87cc-ee2fd57671eb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for timestep in range(5):\n",
        "    plt.subplot(5,2,2*timestep+1)\n",
        "    plt.pcolormesh(predicted_labels[timestep])\n",
        "    if timestep==0:\n",
        "        plt.title('Predictions')\n",
        "    plt.subplot(5,2,2*timestep+2)\n",
        "    plt.pcolormesh(np.reshape(test_labels, (16,48,72))[timestep])\n",
        "    if timestep==0:\n",
        "        plt.title('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d5eae7-a188-40d4-9d51-f21bd64b2dd1",
      "metadata": {
        "id": "e7d5eae7-a188-40d4-9d51-f21bd64b2dd1"
      },
      "source": [
        "At least qualititavely, our models seems to be doing an OK job at predicting some atmospheric rivers (in yellow) and a poor job at predicting tropical cyclones (in turquoise). Let's define and discuss some quantitative measures to expand on this. Firstly, accuracy: how often do predictions=labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d128d440-e54a-4dbb-90fd-9d40ad53d264",
      "metadata": {
        "id": "d128d440-e54a-4dbb-90fd-9d40ad53d264"
      },
      "outputs": [],
      "source": [
        "def accuracy(predicted, true):\n",
        "    return np.sum(predicted==true)/predicted.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f4a565-af1a-49fc-9795-f5497c1d4486",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84f4a565-af1a-49fc-9795-f5497c1d4486",
        "outputId": "1debe209-7e0d-4067-88a1-7956e1157497"
      },
      "outputs": [],
      "source": [
        "print(accuracy(predicted_labels.flatten(), test_labels.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54b5367-0f5c-42ba-b4ba-e5a9adb5ab9e",
      "metadata": {
        "id": "e54b5367-0f5c-42ba-b4ba-e5a9adb5ab9e"
      },
      "source": [
        "The model has very good accuracy on the test set. But how meaningful is this? Consider a baseline model that predicts class 0 for every grid cell. Then since most of the grid cells are indeed class 0, the accuracy will naturally be very high. Indeed, we can compute it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bbf7e0-f41d-4127-9e1a-37c316108f1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4bbf7e0-f41d-4127-9e1a-37c316108f1b",
        "outputId": "cabc571e-2f0b-42ae-ee4a-2ac9764388e0"
      },
      "outputs": [],
      "source": [
        "print(accuracy(np.zeros(test_labels.shape[0]), test_labels.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5956ce08-7d44-4b74-959b-adc19761a179",
      "metadata": {
        "id": "5956ce08-7d44-4b74-959b-adc19761a179"
      },
      "source": [
        "Actually, our model only just exceeds the baseline by this metric. Ideally we would like a metric that captures how often the model is right when it predicts an extreme weather event. If we define a 'positive' to be a given class then we can define **precision** and **recall** as:\n",
        "\n",
        "Precision = #True positives/(#True positives + #False positives)\n",
        "\n",
        "Recall = #True positives/(#True positives + #False negatives)\n",
        "\n",
        "Here a true positive means the model correctly predicted true, and a false positive means the model incorrectly predicted true. If we want to seek a balance between precision and recall we can use the so-called F1 score:\n",
        "\n",
        "F1 = 2 x (precision x recall)/(precision + recall)\n",
        "\n",
        "For an extended discussion on the meaning of these metrics, see this article: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbef40cb-0c14-4e32-a046-6b6c83a63433",
      "metadata": {
        "id": "bbef40cb-0c14-4e32-a046-6b6c83a63433"
      },
      "outputs": [],
      "source": [
        "def precision_label_n(predicted, true, label_n):\n",
        "    tp = ((predicted==label_n).astype(int)*(true==label_n   ).astype(int)).sum()  #number of true positives\n",
        "    fp = ((predicted==label_n).astype(int)*(1-(true==label_n).astype(int))).sum() #number of false positives\n",
        "    return tp/(tp+fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e31e34-a270-441c-8fa5-98d29f2ce7e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5e31e34-a270-441c-8fa5-98d29f2ce7e3",
        "outputId": "352d9b4a-1f3b-417b-9bfc-4bc3b42bad3a"
      },
      "outputs": [],
      "source": [
        "print('Cyclone precision', precision_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=1))\n",
        "print('AR precision'     , precision_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76659706-0edb-4fa8-ae37-07a51cf8576d",
      "metadata": {
        "id": "76659706-0edb-4fa8-ae37-07a51cf8576d"
      },
      "outputs": [],
      "source": [
        "def recall_label_n(predicted, true, label_n):\n",
        "    tp = ((predicted==label_n).astype(int)*(true==label_n     ).astype(int)).sum()  #number of true positives\n",
        "    fn = ((1-(predicted==label_n).astype(int))*((true==label_n).astype(int))).sum() #number of false negatives\n",
        "    return tp/(tp+fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1b3af3-d474-4391-88d7-fa7d61edc61e",
      "metadata": {
        "id": "3e1b3af3-d474-4391-88d7-fa7d61edc61e"
      },
      "outputs": [],
      "source": [
        "print('Cyclone recall'          , recall_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=1))\n",
        "print('Atmospheric river recall', recall_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15203e77-f8bf-415b-a209-e3c7b878de3c",
      "metadata": {
        "id": "15203e77-f8bf-415b-a209-e3c7b878de3c"
      },
      "source": [
        "There is actually a pre-existing library function that will do this for you, for each label. The 'support' is just the number of (true) labels from a given class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa140766-a842-4ea4-b6a2-2b618213fbdb",
      "metadata": {
        "id": "fa140766-a842-4ea4-b6a2-2b618213fbdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "precision, recall, fscore, support = score(test_labels.flatten(), predicted_labels.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d257a2d3-d945-4b75-8c67-551c4d377dc1",
      "metadata": {
        "id": "d257a2d3-d945-4b75-8c67-551c4d377dc1"
      },
      "outputs": [],
      "source": [
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c829f12e-a090-4a05-adce-516e8f0beb37",
      "metadata": {
        "id": "c829f12e-a090-4a05-adce-516e8f0beb37"
      },
      "source": [
        "Here we can clearly see that the fact that the number of instances of each class is seriously skewed is causing some problems for the model. In particular, the recall scores are particularly poor for extreme weather events, i.e. there are a lot of false negatives. This is bad news: we would probably rather have an overcautious model rather than our current very undercautious one (i.e. fails to predict a lot of true extreme weather events).\n",
        "\n",
        "Of course, we should keep in mind that evaluating our model cell-by-cell is also probably not the best measure of performance: certainly the qualitative picture seems to suggest the regional performance on atmospheric rivers is actually OK: i.e. it gets the locations roughly correct (which cell-by-cell metrics of performance do not capture completely)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6438b41-9a05-4f28-bcf3-949abfe8c330",
      "metadata": {
        "id": "f6438b41-9a05-4f28-bcf3-949abfe8c330"
      },
      "source": [
        "# The task\n",
        "We have built a very basic model for predicting atmospheric weather events, and whilst it isn't great, there is an indication that it is picking up on some trends. By copying and adapting the code above, can you improve the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee0c882-ac2d-426f-a928-eb71ed212bbe",
      "metadata": {
        "id": "eee0c882-ac2d-426f-a928-eb71ed212bbe"
      },
      "source": [
        "Some ideas to get you started:\n",
        "\n",
        "1. You could try getting started by simply training the model for more epochs. How much does this improve the performance metrics?\n",
        "2. We have an unbalanced dataset, what happens if you used a weighed loss function? You could also try using a different loss function.\n",
        "3. Next, you might try changing the model design. Why not play around with the number of layers, the number of neurons per layer, and the activation functions?\n",
        "4. What about the optimiser, is the generic Adam the best choice here, what about a larger/smaller learning rate, or a learning rate schedule?\n",
        "5. You might try changing the inputs to the model (look at the data loading notebook from this morning for some ideas). You could also try adding more inputs to the model, changing the filtering procedure, and changing the normalization.\n",
        "6. There is certainly spatial information in the original data that will be useful to a model for detecting extreme weather events. How might you harness this? One simple idea is to actually have latitude and/or longitude values of each grid cell each *as inputs to the model*.\n",
        "7. Another simple idea is to reduce the size of the dataset by filtering out the high latitudes. This will reduce the skew in the number of labels for each class. \n",
        "8. A more complex idea is to create a dataset which predicts a cell label based on data values from that cell and the adjacent cells (note this will require substantially reshaping your training data).\n",
        "9. What about trying a different type of model? You could try a convolutional neural network, or a vision transformer. You could also try a simpler type of ML model given the reasonably small dataset, such as a random forest or a support vector machine.\n",
        "\n",
        "Do feel free to discuss with each other and us if you aren't sure what to do! We do stress that the aim of this notebook is not to create a ground-breaking model for predicting extreme weather events, but rather to get experience constructing a dataset, and playing around with simple machine learning models for making predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ec0a2b-14d8-4dd6-bf77-d70b61358e77",
      "metadata": {
        "id": "01ec0a2b-14d8-4dd6-bf77-d70b61358e77"
      },
      "source": [
        "### Choose inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9319da44-1aa2-4bf0-a6c1-48b12874fadb",
      "metadata": {
        "id": "9319da44-1aa2-4bf0-a6c1-48b12874fadb"
      },
      "outputs": [],
      "source": [
        "input_1 = ds.U850.data\n",
        "input_2 = ds.V850.data\n",
        "input_3 = ds.TMQ.data\n",
        "input_4 = ds.PSL.data\n",
        "# input_5 = ?\n",
        "labels = ds.LABELS.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc09592f-996a-480d-976b-6a9010bec79b",
      "metadata": {
        "id": "fc09592f-996a-480d-976b-6a9010bec79b"
      },
      "source": [
        "### Filter to reduce number of data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fead7a-fd1b-440a-8545-deba0e0c5975",
      "metadata": {
        "id": "24fead7a-fd1b-440a-8545-deba0e0c5975"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage.filters import gaussian_filter\n",
        "filter_input_1 = gaussian_filter(input_1,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_2 = gaussian_filter(input_2,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_3 = gaussian_filter(input_3,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_input_4 = gaussian_filter(input_4,sigma = [0,10,10])[:,::16,::16]\n",
        "filter_labels = labels[:,::16,::16]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f914a20e-c9bd-4a02-80bf-e7c2b39e774f",
      "metadata": {
        "id": "f914a20e-c9bd-4a02-80bf-e7c2b39e774f"
      },
      "source": [
        "### Build training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f010e9-99e5-4fea-9c75-7ebef0cecf95",
      "metadata": {
        "id": "25f010e9-99e5-4fea-9c75-7ebef0cecf95"
      },
      "outputs": [],
      "source": [
        "X_train = np.stack((\n",
        "        filter_input_1[:67].flatten(),\n",
        "        filter_input_2[:67].flatten(),\n",
        "        filter_input_3[:67].flatten(),\n",
        "        filter_input_4[:67].flatten()\n",
        "        ),\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "X_test = np.stack((\n",
        "        filter_input_1[67:].flatten(),\n",
        "        filter_input_2[67:].flatten(),\n",
        "        filter_input_3[67:].flatten(),\n",
        "        filter_input_4[67:].flatten()\n",
        "        ),\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb78a287-76c5-42d6-860c-b60df7b89575",
      "metadata": {
        "id": "bb78a287-76c5-42d6-860c-b60df7b89575"
      },
      "outputs": [],
      "source": [
        "train_labels = filter_labels[:67].flatten()\n",
        "test_labels  = filter_labels[67:].flatten()\n",
        "\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "nb_classes= 3\n",
        "Y_train = to_categorical(train_labels, num_classes=nb_classes)\n",
        "Y_test = to_categorical(test_labels  , num_classes=nb_classes)\n",
        "\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f03442a4-0560-4db1-bc6f-b6d93e21c9e3",
      "metadata": {
        "id": "f03442a4-0560-4db1-bc6f-b6d93e21c9e3"
      },
      "source": [
        "### Normalise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29eb2d8-8ecc-4b30-a0bd-3e27f4ee835a",
      "metadata": {
        "id": "a29eb2d8-8ecc-4b30-a0bd-3e27f4ee835a"
      },
      "outputs": [],
      "source": [
        "mu    = np.mean(X_train, axis=0)\n",
        "sigma = np.std( X_train, axis=0)\n",
        "\n",
        "print(mu, sigma)\n",
        "\n",
        "X_train = (X_train-mu)/sigma #Careful not to run this cell twice\n",
        "X_test  = (X_test -mu)/sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vW3lOVBNjGCe",
      "metadata": {
        "id": "vW3lOVBNjGCe"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset(X_train, Y_train),\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = True\n",
        "    )\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset(X_test , Y_test),\n",
        "    batch_size = batch_size,\n",
        "    shuffle    = False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a977c0-7b9e-42b7-a0df-82f993594675",
      "metadata": {
        "id": "70a977c0-7b9e-42b7-a0df-82f993594675"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893b8710-bd00-48c2-add7-ed70ca11ecb8",
      "metadata": {
        "id": "893b8710-bd00-48c2-add7-ed70ca11ecb8"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_size, nb_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 200),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(200, 60),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(60, nb_classes), # classifying into 3 classes\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# View model architecture and parameters\n",
        "print(summary(Model(4, 3), (1, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bec5ac0-b46c-4256-b67e-17c31c640ea3",
      "metadata": {
        "id": "0bec5ac0-b46c-4256-b67e-17c31c640ea3"
      },
      "outputs": [],
      "source": [
        "net       = Model(4, 3)\n",
        "epochs    = 10\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(net.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b6c1b5-0da5-4c3f-901f-d197014c6e75",
      "metadata": {
        "id": "96b6c1b5-0da5-4c3f-901f-d197014c6e75"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53dccde-4f77-431b-9151-911630bcb5a2",
      "metadata": {
        "id": "a53dccde-4f77-431b-9151-911630bcb5a2"
      },
      "outputs": [],
      "source": [
        "net = fit(\n",
        "    network        = net,\n",
        "    dataloader     = train_dataloader,\n",
        "    val_dataloader = val_dataloader,\n",
        "    criterion      = criterion,\n",
        "    optimiser      = optimiser,\n",
        "    epochs         = epochs,\n",
        "    train_steps    = train_steps\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dde070-d0b3-4fb5-b27f-4cc7fe9b51ac",
      "metadata": {
        "id": "b3dde070-d0b3-4fb5-b27f-4cc7fe9b51ac"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a0a8ff-a0b3-47a4-9856-0421dbcafab7",
      "metadata": {
        "id": "55a0a8ff-a0b3-47a4-9856-0421dbcafab7"
      },
      "outputs": [],
      "source": [
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predicted_classes = net(X_test).numpy()\n",
        "    predicted_labels = np.argmax(predicted_classes, axis=1).reshape((16, 48, 72))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339a828e-1e3f-4f31-ae8f-de73bcd4f055",
      "metadata": {
        "id": "339a828e-1e3f-4f31-ae8f-de73bcd4f055"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for timestep in range(5):\n",
        "\n",
        "    plt.subplot(5,2,2*timestep+1)\n",
        "    plt.pcolormesh(predicted_labels[timestep])\n",
        "    if timestep==0: plt.title('Predictions')\n",
        "\n",
        "    plt.subplot(5,2,2*timestep+2)\n",
        "    plt.pcolormesh(np.reshape(test_labels, (16,48,72))[timestep])\n",
        "    if timestep==0: plt.title('True labels')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd523249-4c89-4f6b-9f29-5be0e6536871",
      "metadata": {
        "id": "cd523249-4c89-4f6b-9f29-5be0e6536871"
      },
      "source": [
        "### Performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7310800a-dab0-4d96-a2d6-ef2208451db4",
      "metadata": {
        "id": "7310800a-dab0-4d96-a2d6-ef2208451db4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "precision, recall, fscore, support = score(test_labels.flatten(), predicted_labels.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc0a798-1a77-4472-9434-2f8b89643e54",
      "metadata": {
        "id": "7cc0a798-1a77-4472-9434-2f8b89643e54"
      },
      "outputs": [],
      "source": [
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
